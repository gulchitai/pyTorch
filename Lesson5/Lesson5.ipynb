{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Курсовая работа\n",
    "\n",
    "Задание по итогам курса\n",
    "\n",
    "Нужно написать приложение, которое будет считывать и выводить кадры с веб-камеры. В процессе считывания определять что перед камерой находится человек, задетектировав его лицо на кадре. После этого, человек показывает жесты руками, а алгоритм должен считать их и определенным образом реагировать на эти жесты.\n",
    "На то, как система будет реагировать на определенные жесты - выбор за вами. Например, на определенный жест (жест пис), система будет здороваться с человеком. На другой, будет делать скриншот экрана. И т.д.\n",
    "Для распознавания жестов, вам надо будет скачать датасет https://www.kaggle.com/gti-upm/leapgestrecog, разработать модель для обучения и обучить эту модель.\n",
    "\n",
    "*(Усложненное задание) Все тоже самое, но воспользоваться этим датасетом:\n",
    "https://fitnessallyapp.com/datasets/jester/v1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install facenet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import glob\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as tt\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из большого датасета возьму только несколько классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = {\n",
    "    \"Swiping Right\": 0,\n",
    "    \"Swiping Right\": 1,\n",
    "    \"No gesture\": 2,\n",
    "    \"Thumb Up\": 3,\n",
    "}\n",
    "digit_to_classname = {0:'Swiping Right', 1:'Swiping Right', 2:'No gesture', 3:'Thumb Up'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Отрисовка всех классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Swiping Right</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-438297e2d990>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mdisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBASE_PATH\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/20bn-jester-v1/68574/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mdisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHTML\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'<h3>Swiping Right</h3>'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimages3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mdisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython import display\n",
    "from IPython.display import HTML\n",
    "\n",
    "images = os.listdir(BASE_PATH + '/20bn-jester-v1/136859/')\n",
    "images2 = os.listdir(BASE_PATH + '/20bn-jester-v1/68574/')\n",
    "images3 = os.listdir(BASE_PATH + '/20bn-jester-v1/20706/')\n",
    "images4 = os.listdir(BASE_PATH + '/20bn-jester-v1/62818/')\n",
    "\n",
    "while True:\n",
    "    for f in images:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(Image(filename=BASE_PATH + '/20bn-jester-v1/136859/'+f))\n",
    "        display.display(HTML('<h3>Thumb Up</h3>'))\n",
    "        time.sleep(0.1)\n",
    "    for f in images2:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(Image(filename=BASE_PATH + '/20bn-jester-v1/68574/'+f))\n",
    "        display.display(HTML('<h3>Swiping Right</h3>'))\n",
    "        time.sleep(0.1)\n",
    "    for f in images3:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(Image(filename=BASE_PATH + '/20bn-jester-v1/20706/'+f))\n",
    "        display.display(HTML('<h3>No gesture</h3>'))\n",
    "        time.sleep(0.1)\n",
    "    for f in images4:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(Image(filename=BASE_PATH + '/20bn-jester-v1/62818/'+f))\n",
    "        display.display(HTML('<h3>Swiping Left</h3>'))\n",
    "        time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "ListDataJpeg = namedtuple('ListDataJpeg', ['id', 'label', 'path'])\n",
    "\n",
    "class JpegDataset(object):\n",
    "\n",
    "    def __init__(self, csv_path_input, csv_path_labels, data_root):\n",
    "        self.classes = self.read_csv_labels(csv_path_labels)\n",
    "        self.classes_dict = self.get_two_way_dict(self.classes)\n",
    "        self.csv_data = self.read_csv_input(csv_path_input, data_root)\n",
    "\n",
    "    def read_csv_input(self, csv_path, data_root):\n",
    "        csv_data = []\n",
    "        with open(csv_path) as csvfile:\n",
    "            csv_reader = csv.reader(csvfile, delimiter=';')\n",
    "            for row in csv_reader:\n",
    "                item = ListDataJpeg(row[0],\n",
    "                                    row[1],\n",
    "                                    os.path.join(data_root, row[0])\n",
    "                                    )\n",
    "                if row[1] in self.classes:\n",
    "                    csv_data.append(item)\n",
    "        return csv_data\n",
    "\n",
    "    def read_csv_labels(self, csv_path):\n",
    "        classes = []\n",
    "        with open(csv_path) as csvfile:\n",
    "            csv_reader = csv.reader(csvfile)\n",
    "            for row in csv_reader:\n",
    "                classes.append(row[0])\n",
    "        return classes\n",
    "\n",
    "    def get_two_way_dict(self, classes):\n",
    "        classes_dict = {}\n",
    "        for i, item in enumerate(classes):\n",
    "            classes_dict[item] = i\n",
    "            classes_dict[i] = item\n",
    "        return classes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_EXTENSIONS = ['.jpg', '.JPG', '.jpeg', '.JPEG']\n",
    "\n",
    "\n",
    "def default_loader(path):\n",
    "    return Image.open(path).convert('RGB')\n",
    "\n",
    "\n",
    "class VideoFolder(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, root, csv_file_input, csv_file_labels, clip_size,\n",
    "                 nclips, step_size, is_val, transform=None,\n",
    "                 loader=default_loader):\n",
    "        self.dataset_object = JpegDataset(csv_file_input, csv_file_labels, root)\n",
    "\n",
    "        self.csv_data = self.dataset_object.csv_data\n",
    "        self.classes = self.dataset_object.classes\n",
    "        self.classes_dict = self.dataset_object.classes_dict\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.loader = loader\n",
    "\n",
    "        self.clip_size = clip_size\n",
    "        self.nclips = nclips\n",
    "        self.step_size = step_size\n",
    "        self.is_val = is_val\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.csv_data[index]\n",
    "        img_paths = self.get_frame_names(item.path)\n",
    "\n",
    "        imgs = []\n",
    "        for img_path in img_paths:\n",
    "            img = self.loader(img_path)\n",
    "            img = self.transform(img)\n",
    "            imgs.append(torch.unsqueeze(img, 0))\n",
    "\n",
    "        target_idx = self.classes_dict[item.label]\n",
    "\n",
    "        # format data to torch\n",
    "        data = torch.cat(imgs)\n",
    "        data = data.permute(1, 0, 2, 3)\n",
    "        return (data, target_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv_data)\n",
    "\n",
    "    def get_frame_names(self, path):\n",
    "        frame_names = []\n",
    "        for ext in IMG_EXTENSIONS:\n",
    "            frame_names.extend(glob.glob(os.path.join(path, \"*\" + ext)))\n",
    "        frame_names = list(sorted(frame_names))\n",
    "        num_frames = len(frame_names)\n",
    "\n",
    "        # set number of necessary frames\n",
    "        if self.nclips > -1:\n",
    "            num_frames_necessary = self.clip_size * self.nclips * self.step_size\n",
    "        else:\n",
    "            num_frames_necessary = num_frames\n",
    "\n",
    "        # pick frames\n",
    "        offset = 0\n",
    "        if num_frames_necessary > num_frames:\n",
    "            # pad last frame if video is shorter than necessary\n",
    "            frame_names += [frame_names[-1]] * (num_frames_necessary - num_frames)\n",
    "        elif num_frames_necessary < num_frames:\n",
    "            # If there are more frames, then sample starting offset\n",
    "            diff = (num_frames - num_frames_necessary)\n",
    "            # Temporal augmentation\n",
    "            if not self.is_val:\n",
    "                offset = np.random.randint(0, diff)\n",
    "        frame_names = frame_names[offset:num_frames_necessary +\n",
    "                                  offset:self.step_size]\n",
    "        return frame_names"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "transform = Compose([\n",
    "                        CenterCrop(84),\n",
    "                        ToTensor(),\n",
    "                        # Normalize(\n",
    "                        #     mean=[0.485, 0.456, 0.406],\n",
    "                        #     std=[0.229, 0.224, 0.225])\n",
    "                        ])\n",
    "loader = VideoFolder(root=\"/hdd/20bn-datasets/20bn-jester-v1/\",\n",
    "                         csv_file_input=\"csv_files/jester-v1-validation.csv\",\n",
    "                         csv_file_labels=\"csv_files/jester-v1-labels.csv\",\n",
    "                         clip_size=18,\n",
    "                         nclips=1,\n",
    "                         step_size=2,\n",
    "                         is_val=False,\n",
    "                         transform=transform,\n",
    "                         loader=default_loader)\n",
    "    # data_item, target_idx = loader[0]\n",
    "    # save_images_for_debug(\"input_images\", data_item.unsqueeze(0))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        loader,\n",
    "        batch_size=10, shuffle=False,\n",
    "        num_workers=5, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = tt.Compose([\n",
    "        tt.CenterCrop(84),\n",
    "        tt.ToTensor(),\n",
    "        tt.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                  std=[0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = VideoFolder(root=\"./data/20bn-jester-v1-short\",\n",
    "                             csv_file_input='./data/jester-v1-train-short.csv',\n",
    "                             csv_file_labels=\"./data/jester-v1-labels-short.csv\",\n",
    "                             clip_size=18,\n",
    "                             nclips=1,\n",
    "                             step_size=2,\n",
    "                             is_val=False,\n",
    "                             transform=transform,\n",
    "                             )\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_data,\n",
    "        batch_size=10, shuffle=True,\n",
    "        num_workers=0, pin_memory=True,\n",
    "        drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1689"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = VideoFolder(root=\"./data/20bn-jester-v1-short\",\n",
    "                           csv_file_input=\"./data/jester-v1-validation-short.csv\",\n",
    "                           csv_file_labels=\"./data/jester-v1-labels-short.csv\",\n",
    "                           clip_size=18,\n",
    "                           nclips=1,\n",
    "                           step_size=2,\n",
    "                           is_val=True,\n",
    "                           transform=transform,\n",
    "                           )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        val_data,\n",
    "        batch_size=10, shuffle=False,\n",
    "        num_workers=0, pin_memory=True,\n",
    "        drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvColumn(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvColumn, self).__init__()\n",
    "\n",
    "        self.conv_layer1 = self._make_conv_layer(3, 64, (1, 2, 2), (1, 2, 2))\n",
    "        self.conv_layer2 = self._make_conv_layer(64, 128, (2, 2, 2), (2, 2, 2))\n",
    "        self.conv_layer3 = self._make_conv_layer(\n",
    "            128, 256, (2, 2, 2), (2, 2, 2))\n",
    "        self.conv_layer4 = self._make_conv_layer(\n",
    "            256, 256, (2, 2, 2), (2, 2, 2))\n",
    "\n",
    "        self.fc5 = nn.Linear(12800, 512)\n",
    "        self.fc5_act = nn.ELU()\n",
    "        self.fc6 = nn.Linear(512, 4)\n",
    "\n",
    "    def _make_conv_layer(self, in_c, out_c, pool_size, stride):\n",
    "        conv_layer = nn.Sequential(\n",
    "            nn.Conv3d(in_c, out_c, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm3d(out_c),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool3d(pool_size, stride=stride, padding=0)\n",
    "        )\n",
    "        return conv_layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer1(x)\n",
    "        x = self.conv_layer2(x)\n",
    "        x = self.conv_layer3(x)\n",
    "        x = self.conv_layer4(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.fc5(x)\n",
    "        x = self.fc5_act(x)\n",
    "\n",
    "        x = self.fc6(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvColumn(\n",
       "  (conv_layer1): Sequential(\n",
       "    (0): Conv3d(3, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ELU(alpha=1.0)\n",
       "    (3): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_layer2): Sequential(\n",
       "    (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ELU(alpha=1.0)\n",
       "    (3): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_layer3): Sequential(\n",
       "    (0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ELU(alpha=1.0)\n",
       "    (3): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_layer4): Sequential(\n",
       "    (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ELU(alpha=1.0)\n",
       "    (3): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc5): Linear(in_features=12800, out_features=512, bias=True)\n",
       "  (fc5_act): ELU(alpha=1.0)\n",
       "  (fc6): Linear(in_features=512, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ConvColumn(4)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define optimizer\n",
    "epochs = 3\n",
    "max_lr = 0.008\n",
    "lr = 0.001\n",
    "last_lr = 0.00001\n",
    "momentum = 0.9\n",
    "weight_decay = 0.00001\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr,\n",
    "                                momentum=momentum,\n",
    "                                weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "epochs = 3\n",
    "max_lr = 0.008\n",
    "grad_clip = 0.1\n",
    "weight_decay = 1e-4\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr, momentum=momentum, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
    "                                                steps_per_epoch=len(train_loader))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1, Loss: 0.26934595329687\n",
      "Spend time for 100 image\\\\1488.518367767334 sec\n",
      "Train Epoch: 1, Loss: 0.2713235933892429\n",
      "Spend time for 100 image\\\\1497.2618458271027 sec\n",
      "Train Epoch: 1, Loss: 0.24831831659190357\n",
      "Spend time for 100 image\\\\1498.301724433899 sec\n",
      "Train Epoch: 1, Loss: 0.28813568005338314\n",
      "Spend time for 100 image\\\\1502.8221185207367 sec\n",
      "Train Epoch: 1, Loss: 0.24045562885701657\n",
      "Spend time for 100 image\\\\1514.1790232658386 sec\n",
      "Train Epoch: 1, Loss: 0.23529196393676102\n",
      "Spend time for 100 image\\\\1507.360762834549 sec\n",
      "Train Epoch: 1, Loss: 0.2706247170269489\n",
      "Spend time for 100 image\\\\1509.9908180236816 sec\n",
      "Train Epoch: 1, Loss: 0.27925469645299017\n",
      "Spend time for 100 image\\\\1508.2095522880554 sec\n",
      "Train Epoch: 1, Loss: 0.26760224368423224\n",
      "Spend time for 100 image\\\\1503.4406611919403 sec\n",
      "Train Epoch: 1, Loss: 0.29440789749845864\n",
      "Spend time for 100 image\\\\1507.4525842666626 sec\n",
      "Train Epoch: 1, Loss: 0.2732845742907375\n",
      "Spend time for 100 image\\\\1512.9527401924133 sec\n",
      "Train Epoch: 1, Loss: 0.24614408462308346\n",
      "Spend time for 100 image\\\\1512.791194677353 sec\n",
      "Train Epoch: 1, Loss: 0.22692681124433875\n",
      "Spend time for 100 image\\\\1504.0569443702698 sec\n",
      "Train Epoch: 1, Loss: 0.27954717142507435\n",
      "Spend time for 100 image\\\\1505.2451157569885 sec\n",
      "Train Epoch: 1, Loss: 0.2842248605191708\n",
      "Spend time for 100 image\\\\1516.3218762874603 sec\n",
      "Train Epoch: 1, Loss: 0.2566393321752548\n",
      "Spend time for 100 image\\\\1541.3536965847015 sec\n",
      "Epoch 1, loss:  0.2665076715355577\n",
      "Train Epoch: 2, Loss: 0.2561963388696313\n",
      "Spend time for 100 image\\\\1502.0285186767578 sec\n",
      "Train Epoch: 2, Loss: 0.29583563746884467\n",
      "Spend time for 100 image\\\\1573.1938736438751 sec\n",
      "Train Epoch: 2, Loss: 0.23774764431640505\n",
      "Spend time for 100 image\\\\1632.7791712284088 sec\n",
      "Train Epoch: 2, Loss: 0.21823837781324984\n",
      "Spend time for 100 image\\\\1491.5921881198883 sec\n",
      "Train Epoch: 2, Loss: 0.27617582474835217\n",
      "Spend time for 100 image\\\\1486.673122882843 sec\n",
      "Train Epoch: 2, Loss: 0.28902047941461206\n",
      "Spend time for 100 image\\\\1507.6196727752686 sec\n",
      "Train Epoch: 2, Loss: 0.2806048938818276\n",
      "Spend time for 100 image\\\\1500.0685951709747 sec\n",
      "Train Epoch: 2, Loss: 0.30078016288578513\n",
      "Spend time for 100 image\\\\1451.0420503616333 sec\n",
      "Train Epoch: 2, Loss: 0.2561148611083627\n",
      "Spend time for 100 image\\\\1453.148175239563 sec\n",
      "Train Epoch: 2, Loss: 0.307521514846012\n",
      "Spend time for 100 image\\\\1453.5891885757446 sec\n",
      "Train Epoch: 2, Loss: 0.2794333984144032\n",
      "Spend time for 100 image\\\\1451.7261395454407 sec\n",
      "Train Epoch: 2, Loss: 0.2466527456557378\n",
      "Spend time for 100 image\\\\1455.2912108898163 sec\n",
      "Train Epoch: 2, Loss: 0.2767462517414242\n",
      "Spend time for 100 image\\\\1464.6294181346893 sec\n",
      "Train Epoch: 2, Loss: 0.2425020562671125\n",
      "Spend time for 100 image\\\\1468.414644241333 sec\n",
      "Train Epoch: 2, Loss: 0.24269382402300835\n",
      "Spend time for 100 image\\\\1465.235728263855 sec\n",
      "Train Epoch: 2, Loss: 0.26179316287860277\n",
      "Spend time for 100 image\\\\1464.7997076511383 sec\n",
      "Epoch 2, loss:  0.26550567031150396\n",
      "Train Epoch: 3, Loss: 0.2785033201240003\n",
      "Spend time for 100 image\\\\1442.5692255496979 sec\n",
      "Train Epoch: 3, Loss: 0.2343477395642549\n",
      "Spend time for 100 image\\\\1462.510561466217 sec\n",
      "Train Epoch: 3, Loss: 0.22566015872173012\n",
      "Spend time for 100 image\\\\1466.4974806308746 sec\n",
      "Train Epoch: 3, Loss: 0.29577032413333654\n",
      "Spend time for 100 image\\\\1466.091388463974 sec\n",
      "Train Epoch: 3, Loss: 0.27688800180330875\n",
      "Spend time for 100 image\\\\1462.417568206787 sec\n",
      "Train Epoch: 3, Loss: 0.2737055676896125\n",
      "Spend time for 100 image\\\\1461.4944818019867 sec\n",
      "Train Epoch: 3, Loss: 0.23334400328807534\n",
      "Spend time for 100 image\\\\1464.7793776988983 sec\n",
      "Train Epoch: 3, Loss: 0.2514130244776607\n",
      "Spend time for 100 image\\\\1491.6111478805542 sec\n",
      "Train Epoch: 3, Loss: 0.2558959640096873\n",
      "Spend time for 100 image\\\\1503.2821371555328 sec\n",
      "Train Epoch: 3, Loss: 0.29598664173856376\n",
      "Spend time for 100 image\\\\1529.5433580875397 sec\n",
      "Train Epoch: 3, Loss: 0.2690832254104316\n",
      "Spend time for 100 image\\\\1572.0914759635925 sec\n",
      "Train Epoch: 3, Loss: 0.25184638938866555\n",
      "Spend time for 100 image\\\\1599.499594926834 sec\n",
      "Train Epoch: 3, Loss: 0.2450364767666906\n",
      "Spend time for 100 image\\\\2313.3865988254547 sec\n",
      "Train Epoch: 3, Loss: 0.23668196268379688\n",
      "Spend time for 100 image\\\\1693.1841609477997 sec\n",
      "Train Epoch: 3, Loss: 0.2520238980744034\n",
      "Spend time for 100 image\\\\1568.5882287025452 sec\n",
      "Train Epoch: 3, Loss: 0.2602108289953321\n",
      "Spend time for 100 image\\\\1434.83509683609 sec\n",
      "Epoch 3, loss:  0.25861398841581396\n"
     ]
    }
   ],
   "source": [
    "#epoch_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    time1 = time.time()\n",
    "    running_loss = 0.0\n",
    "    epoch_loss = []\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data, labels = Variable(data), Variable(labels)\n",
    "        #data = data.cuda()\n",
    "        #labels = labels.cuda()\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(data)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        epoch_loss.append(loss.item())\n",
    "        if (batch_idx+1) % 100 == 99:\n",
    "            print(f'Train Epoch: {epoch+1}, Loss: {running_loss/100}')\n",
    "            time2 = time.time()\n",
    "            print(f'Spend time for 100 image\\\\\\{time2 - time1} sec')\n",
    "            time1 = time.time()\n",
    "            running_loss = 0.0\n",
    "    print(f'Epoch {epoch+1}, loss: ', np.mean(epoch_loss))\n",
    "    epoch_losses.append(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'losses vs. No. of epochs')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = [np.mean(loss) for loss in epoch_losses]\n",
    "plt.plot(losses, '-x')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('losses')\n",
    "plt.title('losses vs. No. of epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './models/proba3.pth')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with torch.no_grad():\n",
    "    for i, data in enumerate(val_loader):\n",
    "        images, labels = data\n",
    "        images, labels = Variable(images,volatile=True), Variable(labels,volatile=True)\n",
    "        outputs = model(images)\n",
    "        print(type(outputs[0].argmax().numpy()))\n",
    "        #print(digit_to_classname[outputs[0].argmax().numpy()])\n",
    "        #plt.title(f'gaused - {digit_to_classname(outputs[0].argmax())}, groud truth - {digit_to_classname(labels[0])}')\n",
    "        #plt.imshow(images[0].cpu().squeeze(), cmap='gray')\n",
    "        #plt.show()\n",
    "        if i>10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return digit_to_classname[category_i], category_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "2 1\n",
      "3 2\n",
      "4 3\n",
      "5 4\n",
      "6 5\n",
      "7 6\n",
      "8 7\n",
      "9 8\n",
      "10 9\n",
      "11 10\n",
      "12 11\n",
      "13 12\n",
      "14 13\n",
      "15 14\n",
      "16 15\n",
      "17 16\n",
      "18 17\n",
      "19 18\n",
      "20 19\n",
      "21 20\n",
      "22 21\n",
      "23 22\n",
      "24 23\n",
      "25 23\n",
      "26 23\n",
      "27 24\n",
      "28 25\n",
      "29 26\n",
      "30 26\n",
      "31 27\n",
      "32 27\n",
      "33 28\n",
      "34 29\n",
      "35 30\n",
      "36 31\n",
      "37 32\n",
      "38 33\n",
      "39 34\n",
      "40 35\n",
      "41 36\n",
      "42 37\n",
      "43 38\n",
      "44 38\n",
      "45 39\n",
      "46 40\n",
      "47 41\n",
      "48 42\n",
      "49 43\n",
      "50 44\n",
      "51 45\n",
      "52 46\n",
      "53 46\n",
      "54 47\n",
      "55 48\n",
      "56 49\n",
      "57 50\n",
      "58 51\n",
      "59 52\n",
      "60 53\n",
      "61 54\n",
      "62 55\n",
      "63 56\n",
      "64 57\n",
      "65 58\n",
      "66 59\n",
      "67 60\n",
      "68 61\n",
      "69 61\n",
      "70 62\n",
      "71 63\n",
      "72 64\n",
      "73 65\n",
      "74 66\n",
      "75 67\n",
      "76 68\n",
      "77 69\n",
      "78 69\n",
      "79 70\n",
      "80 71\n",
      "81 72\n",
      "82 73\n",
      "83 74\n",
      "84 75\n",
      "85 76\n",
      "86 77\n",
      "87 78\n",
      "88 79\n",
      "89 80\n",
      "90 81\n",
      "91 82\n",
      "92 82\n",
      "93 83\n",
      "94 84\n",
      "95 85\n",
      "96 86\n",
      "97 86\n",
      "98 87\n",
      "99 87\n",
      "100 88\n",
      "101 89\n",
      "102 90\n",
      "103 91\n",
      "104 91\n",
      "105 92\n",
      "106 93\n",
      "107 94\n",
      "108 95\n",
      "109 96\n",
      "110 97\n",
      "111 98\n",
      "112 99\n",
      "113 100\n",
      "114 101\n",
      "115 102\n",
      "116 103\n",
      "117 104\n",
      "118 105\n",
      "119 105\n",
      "120 106\n",
      "121 107\n",
      "122 107\n",
      "123 108\n",
      "124 109\n",
      "125 110\n",
      "126 111\n",
      "127 112\n",
      "128 112\n",
      "129 113\n",
      "130 114\n",
      "131 115\n",
      "132 116\n",
      "133 117\n",
      "134 118\n",
      "135 119\n",
      "136 120\n",
      "137 121\n",
      "138 122\n",
      "139 123\n",
      "140 124\n",
      "141 125\n",
      "142 125\n",
      "143 126\n",
      "144 127\n",
      "145 128\n",
      "146 129\n",
      "147 130\n",
      "148 131\n",
      "149 132\n",
      "150 133\n",
      "151 134\n",
      "152 135\n",
      "153 136\n",
      "154 137\n",
      "155 138\n",
      "156 139\n",
      "157 140\n",
      "158 141\n",
      "159 142\n",
      "160 143\n",
      "161 144\n",
      "162 145\n",
      "163 146\n",
      "164 147\n",
      "165 148\n",
      "166 148\n",
      "167 149\n",
      "168 150\n",
      "169 151\n",
      "170 152\n",
      "171 153\n",
      "172 153\n",
      "173 154\n",
      "174 155\n",
      "175 156\n",
      "176 157\n",
      "177 158\n",
      "178 159\n",
      "179 160\n",
      "180 161\n",
      "181 162\n",
      "182 163\n",
      "183 164\n",
      "184 165\n",
      "185 166\n",
      "186 167\n",
      "187 168\n",
      "188 169\n",
      "189 169\n",
      "190 170\n",
      "191 171\n",
      "192 172\n",
      "193 173\n",
      "194 173\n",
      "195 173\n",
      "196 174\n",
      "197 175\n",
      "198 176\n",
      "199 177\n",
      "200 178\n",
      "201 179\n",
      "202 179\n",
      "203 180\n",
      "204 180\n",
      "205 181\n",
      "206 182\n",
      "Accuracy of the network:   88.3495145631068\n"
     ]
    }
   ],
   "source": [
    "#total = 0\n",
    "right = 0\n",
    "counter = 0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(val_loader, 0):\n",
    "        counter = counter + 1\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)   \n",
    "        output = model(inputs)\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        category = digit_to_classname[int(labels[0])]\n",
    "        \n",
    "        if guess == category:\n",
    "            right = right + 1\n",
    "        print(counter, right)\n",
    "\n",
    "\n",
    "print('Accuracy of the network:  ',  (100 * right / counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvColumn(\n",
       "  (conv_layer1): Sequential(\n",
       "    (0): Conv3d(3, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ELU(alpha=1.0)\n",
       "    (3): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_layer2): Sequential(\n",
       "    (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ELU(alpha=1.0)\n",
       "    (3): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_layer3): Sequential(\n",
       "    (0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ELU(alpha=1.0)\n",
       "    (3): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_layer4): Sequential(\n",
       "    (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ELU(alpha=1.0)\n",
       "    (3): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc5): Linear(in_features=12800, out_features=512, bias=True)\n",
       "  (fc5_act): ELU(alpha=1.0)\n",
       "  (fc6): Linear(in_features=512, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n",
      "Something wrong im draw function!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "# Создаем объект для считывания потока с веб-камеры(обычно вебкамера идет под номером 0. иногда 1)\n",
    "cap = cv2.VideoCapture(0)  \n",
    "\n",
    "# Класс детектирования и обработки лица с веб-камеры \n",
    "class GestureDetector(object):\n",
    "\n",
    "    def __init__(self, mtcnn):\n",
    "        self.mtcnn = mtcnn\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.emodel = ConvColumn(4).to(self.device)\n",
    "        self.emodel.load_state_dict(torch.load('./models/proba3(1-6).pth'))\n",
    "        self.emodel.eval()\n",
    "\n",
    "    # Функция рисования найденных параметров на кадре\n",
    "    def _draw(self, frame, boxes, probs, landmarks):\n",
    "        try:\n",
    "            for box, prob, ld in zip(boxes, probs, landmarks):\n",
    "                # Рисуем обрамляющий прямоугольник лица на кадре\n",
    "                cv2.rectangle(frame,\n",
    "                              (box[0], box[1]),\n",
    "                              (box[2], box[3]),\n",
    "                              (0, 0, 255),\n",
    "                              thickness=2)\n",
    "\n",
    "                # пишем на кадре какая эмоция распознана\n",
    "                #cv2.putText(frame, \n",
    "                #    emotion, (box[2], box[3]), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "                # Рисуем особенные точки\n",
    "                #cv2.circle(frame, tuple(ld[0]), 5, (0, 0, 255), -1)\n",
    "                #cv2.circle(frame, tuple(ld[1]), 5, (0, 0, 255), -1)\n",
    "                #cv2.circle(frame, tuple(ld[2]), 5, (0, 0, 255), -1)\n",
    "                #cv2.circle(frame, tuple(ld[3]), 5, (0, 0, 255), -1)\n",
    "                #cv2.circle(frame, tuple(ld[4]), 5, (0, 0, 255), -1)\n",
    "        except:\n",
    "            print('Something wrong im draw function!')\n",
    "\n",
    "        return frame\n",
    "    \n",
    "    # Функция для вырезания лиц с кадра\n",
    "    @staticmethod\n",
    "    def crop_faces(frame, boxes):\n",
    "        faces = []\n",
    "        for i, box in enumerate(boxes):\n",
    "            faces.append(frame[int(box[1]):int(box[3]), \n",
    "                int(box[0]):int(box[2])])\n",
    "        return faces\n",
    "    \n",
    "    @staticmethod\n",
    "    def digit_to_classname(digit):\n",
    "        if digit == 0:\n",
    "            return 'Swiping Right'\n",
    "        elif digit == 1:\n",
    "            return 'Swiping Left'\n",
    "        elif digit == 2:\n",
    "            return 'No gesture'\n",
    "        elif digit == 3:\n",
    "            return 'Thumb Up'\n",
    "       \n",
    "    # Функция в которой будет происходить процесс считывания и обработки каждого кадра\n",
    "    def run(self):              \n",
    "        # Заходим в бесконечный цикл\n",
    "        while True:\n",
    "            # Считываем каждый новый кадр - frame\n",
    "            # ret - логическая переменая. Смысл - считали ли мы кадр с потока или нет\n",
    "            ret, frame = cap.read()\n",
    "            try:\n",
    "                # детектируем расположение лица на кадре, вероятности на сколько это лицо\n",
    "                # и особенные точки лица\n",
    "                boxes, probs, landmarks = self.mtcnn.detect(frame, landmarks=True)\n",
    "                \n",
    "                # Вырезаем лицо из кадра\n",
    "                #face = self.crop_faces(frame, boxes)[0]\n",
    "                # Меняем размер изображения лица для входа в нейронную сеть\n",
    "                #face = cv2.resize(face,(48,48))\n",
    "                # Превращаем в 1-канальное серое изображение\n",
    "                #face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "                # Превращаем numpy-картинку вырезанного лица в pytorch-тензор\n",
    "                #torch_face = torch.from_numpy(face).unsqueeze(0).to(self.device).float()\n",
    "                # Загужаем наш тензор лица в нейронную сеть и получаем предсказание\n",
    "                #emotion = self.emodel(torch_face[None, ...])\n",
    "                # Интерпретируем предсказание как строку нашей эмоции\n",
    "                #emotion = self.digit_to_classname(emotion.argmax())\n",
    "\n",
    "                # Рисуем на кадре\n",
    "                self._draw(frame, boxes, probs, landmarks)\n",
    "\n",
    "            except:\n",
    "                print('Something wrong im main cycle!')\n",
    "\n",
    "            # Показываем кадр в окне, и назвываем его(окно) - 'Face Detection'\n",
    "            cv2.imshow('Face Detection', frame)\n",
    "            \n",
    "            # Функция, которая проверяет нажатие на клавишу 'q'\n",
    "            # Если нажатие произошло - выход из цикла. Конец работы приложения\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "                \n",
    "        # Очищаем все объекты opencv, что мы создали\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        \n",
    "# Загружаем мтцнн\n",
    "mtcnn = MTCNN()\n",
    "# Создаем объект нашего класса приложения\n",
    "gsd = GestureDetector(mtcnn)\n",
    "# Запускаем\n",
    "gsd.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
