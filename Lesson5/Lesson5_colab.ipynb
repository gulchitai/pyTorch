{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Lesson5.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnoeoAQiEm4j"
      },
      "source": [
        "### Курсовая работа\n",
        "\n",
        "Задание по итогам курса\n",
        "\n",
        "Нужно написать приложение, которое будет считывать и выводить кадры с веб-камеры. В процессе считывания определять что перед камерой находится человек, задетектировав его лицо на кадре. После этого, человек показывает жесты руками, а алгоритм должен считать их и определенным образом реагировать на эти жесты.\n",
        "На то, как система будет реагировать на определенные жесты - выбор за вами. Например, на определенный жест (жест пис), система будет здороваться с человеком. На другой, будет делать скриншот экрана. И т.д.\n",
        "Для распознавания жестов, вам надо будет скачать датасет https://www.kaggle.com/gti-upm/leapgestrecog, разработать модель для обучения и обучить эту модель.\n",
        "\n",
        "*(Усложненное задание) Все тоже самое, но воспользоваться этим датасетом:\n",
        "https://fitnessallyapp.com/datasets/jester/v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VhgSQEIEm4l"
      },
      "source": [
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdsLlnENEm4n"
      },
      "source": [
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import glob\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as tt\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import time\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mN1C7CisEm4z"
      },
      "source": [
        "Из большого датасета возьму только несколько классов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NstokB_3Em41"
      },
      "source": [
        "LABELS = {\n",
        "    \"Swiping Right\": 0,\n",
        "    \"Swiping Left\": 1,\n",
        "    \"No gesture\": 2,\n",
        "    \"Thumb Up\": 3,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFAKD8k_Em4-"
      },
      "source": [
        "### Отрисовка всех классов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cksWpurTEm5A",
        "outputId": "8fc5120c-9a2b-42a6-9b1d-909d69c7149d"
      },
      "source": [
        "from IPython import display\n",
        "from IPython.display import HTML\n",
        "\n",
        "images = os.listdir(BASE_PATH + '/20bn-jester-v1/136859/')\n",
        "images2 = os.listdir(BASE_PATH + '/20bn-jester-v1/68574/')\n",
        "images3 = os.listdir(BASE_PATH + '/20bn-jester-v1/20706/')\n",
        "images4 = os.listdir(BASE_PATH + '/20bn-jester-v1/62818/')\n",
        "\n",
        "while True:\n",
        "    for f in images:\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(Image(filename=BASE_PATH + '/20bn-jester-v1/136859/'+f))\n",
        "        display.display(HTML('<h3>Thumb Up</h3>'))\n",
        "        time.sleep(0.1)\n",
        "    for f in images2:\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(Image(filename=BASE_PATH + '/20bn-jester-v1/68574/'+f))\n",
        "        display.display(HTML('<h3>Swiping Right</h3>'))\n",
        "        time.sleep(0.1)\n",
        "    for f in images3:\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(Image(filename=BASE_PATH + '/20bn-jester-v1/20706/'+f))\n",
        "        display.display(HTML('<h3>No gesture</h3>'))\n",
        "        time.sleep(0.1)\n",
        "    for f in images4:\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(Image(filename=BASE_PATH + '/20bn-jester-v1/62818/'+f))\n",
        "        display.display(HTML('<h3>Swiping Left</h3>'))\n",
        "        time.sleep(0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Swiping Right</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-19-438297e2d990>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mdisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBASE_PATH\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/20bn-jester-v1/68574/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mdisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHTML\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'<h3>Swiping Right</h3>'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimages3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mdisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3W9uKrjNEm5M"
      },
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "ListDataJpeg = namedtuple('ListDataJpeg', ['id', 'label', 'path'])\n",
        "\n",
        "class JpegDataset(object):\n",
        "\n",
        "    def __init__(self, csv_path_input, csv_path_labels, data_root):\n",
        "        self.classes = self.read_csv_labels(csv_path_labels)\n",
        "        self.classes_dict = self.get_two_way_dict(self.classes)\n",
        "        self.csv_data = self.read_csv_input(csv_path_input, data_root)\n",
        "\n",
        "    def read_csv_input(self, csv_path, data_root):\n",
        "        csv_data = []\n",
        "        with open(csv_path) as csvfile:\n",
        "            csv_reader = csv.reader(csvfile, delimiter=';')\n",
        "            for row in csv_reader:\n",
        "                item = ListDataJpeg(row[0],\n",
        "                                    row[1],\n",
        "                                    os.path.join(data_root, row[0])\n",
        "                                    )\n",
        "                if row[1] in self.classes:\n",
        "                    csv_data.append(item)\n",
        "        return csv_data\n",
        "\n",
        "    def read_csv_labels(self, csv_path):\n",
        "        classes = []\n",
        "        with open(csv_path) as csvfile:\n",
        "            csv_reader = csv.reader(csvfile)\n",
        "            for row in csv_reader:\n",
        "                classes.append(row[0])\n",
        "        return classes\n",
        "\n",
        "    def get_two_way_dict(self, classes):\n",
        "        classes_dict = {}\n",
        "        for i, item in enumerate(classes):\n",
        "            classes_dict[item] = i\n",
        "            classes_dict[i] = item\n",
        "        return classes_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8Mzu9THEm5V"
      },
      "source": [
        "IMG_EXTENSIONS = ['.jpg', '.JPG', '.jpeg', '.JPEG']\n",
        "\n",
        "\n",
        "def default_loader(path):\n",
        "    return Image.open(path).convert('RGB')\n",
        "\n",
        "\n",
        "class VideoFolder(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, root, csv_file_input, csv_file_labels, clip_size,\n",
        "                 nclips, step_size, is_val, transform=None,\n",
        "                 loader=default_loader):\n",
        "        self.dataset_object = JpegDataset(csv_file_input, csv_file_labels, root)\n",
        "\n",
        "        self.csv_data = self.dataset_object.csv_data\n",
        "        self.classes = self.dataset_object.classes\n",
        "        self.classes_dict = self.dataset_object.classes_dict\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.loader = loader\n",
        "\n",
        "        self.clip_size = clip_size\n",
        "        self.nclips = nclips\n",
        "        self.step_size = step_size\n",
        "        self.is_val = is_val\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        item = self.csv_data[index]\n",
        "        img_paths = self.get_frame_names(item.path)\n",
        "\n",
        "        imgs = []\n",
        "        for img_path in img_paths:\n",
        "            img = self.loader(img_path)\n",
        "            img = self.transform(img)\n",
        "            imgs.append(torch.unsqueeze(img, 0))\n",
        "\n",
        "        target_idx = self.classes_dict[item.label]\n",
        "\n",
        "        # format data to torch\n",
        "        data = torch.cat(imgs)\n",
        "        data = data.permute(1, 0, 2, 3)\n",
        "        return (data, target_idx)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.csv_data)\n",
        "\n",
        "    def get_frame_names(self, path):\n",
        "        frame_names = []\n",
        "        for ext in IMG_EXTENSIONS:\n",
        "            frame_names.extend(glob.glob(os.path.join(path, \"*\" + ext)))\n",
        "        frame_names = list(sorted(frame_names))\n",
        "        num_frames = len(frame_names)\n",
        "\n",
        "        # set number of necessary frames\n",
        "        if self.nclips > -1:\n",
        "            num_frames_necessary = self.clip_size * self.nclips * self.step_size\n",
        "        else:\n",
        "            num_frames_necessary = num_frames\n",
        "\n",
        "        # pick frames\n",
        "        offset = 0\n",
        "        if num_frames_necessary > num_frames:\n",
        "            # pad last frame if video is shorter than necessary\n",
        "            frame_names += [frame_names[-1]] * (num_frames_necessary - num_frames)\n",
        "        elif num_frames_necessary < num_frames:\n",
        "            # If there are more frames, then sample starting offset\n",
        "            diff = (num_frames - num_frames_necessary)\n",
        "            # Temporal augmentation\n",
        "            if not self.is_val:\n",
        "                offset = np.random.randint(0, diff)\n",
        "        frame_names = frame_names[offset:num_frames_necessary +\n",
        "                                  offset:self.step_size]\n",
        "        return frame_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEK61g0XEm5d"
      },
      "source": [
        "transform = Compose([\n",
        "                        CenterCrop(84),\n",
        "                        ToTensor(),\n",
        "                        # Normalize(\n",
        "                        #     mean=[0.485, 0.456, 0.406],\n",
        "                        #     std=[0.229, 0.224, 0.225])\n",
        "                        ])\n",
        "loader = VideoFolder(root=\"/hdd/20bn-datasets/20bn-jester-v1/\",\n",
        "                         csv_file_input=\"csv_files/jester-v1-validation.csv\",\n",
        "                         csv_file_labels=\"csv_files/jester-v1-labels.csv\",\n",
        "                         clip_size=18,\n",
        "                         nclips=1,\n",
        "                         step_size=2,\n",
        "                         is_val=False,\n",
        "                         transform=transform,\n",
        "                         loader=default_loader)\n",
        "    # data_item, target_idx = loader[0]\n",
        "    # save_images_for_debug(\"input_images\", data_item.unsqueeze(0))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "        loader,\n",
        "        batch_size=10, shuffle=False,\n",
        "        num_workers=5, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MD2NEBlKEm5f"
      },
      "source": [
        "transform = tt.Compose([\n",
        "        tt.CenterCrop(84),\n",
        "        tt.ToTensor(),\n",
        "        tt.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                  std=[0.229, 0.224, 0.225])\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu8komCdEm5p"
      },
      "source": [
        "train_data = VideoFolder(root=\"./data/20bn-jester-v1-short\",\n",
        "                             csv_file_input='./data/jester-v1-train-short.csv',\n",
        "                             csv_file_labels=\"./data/jester-v1-labels-short.csv\",\n",
        "                             clip_size=18,\n",
        "                             nclips=1,\n",
        "                             step_size=2,\n",
        "                             is_val=False,\n",
        "                             transform=transform,\n",
        "                             )\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "        train_data,\n",
        "        batch_size=10, shuffle=True,\n",
        "        num_workers=0, pin_memory=True,\n",
        "        drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jGxJrK5Em7-",
        "outputId": "87d4825d-79f9-4480-f540-a1e189a4efa7"
      },
      "source": [
        "len(train_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1689"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yavRPq-zEm8U"
      },
      "source": [
        "val_data = VideoFolder(root=\"./data/20bn-jester-v1-short\",\n",
        "                           csv_file_input=\"./data/jester-v1-validation-short.csv\",\n",
        "                           csv_file_labels=\"./data/jester-v1-labels-short.csv\",\n",
        "                           clip_size=18,\n",
        "                           nclips=1,\n",
        "                           step_size=2,\n",
        "                           is_val=True,\n",
        "                           transform=transform,\n",
        "                           )\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "        val_data,\n",
        "        batch_size=10, shuffle=False,\n",
        "        num_workers=0, pin_memory=True,\n",
        "        drop_last=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_izOhXzyEm8d"
      },
      "source": [
        "class ConvColumn(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes):\n",
        "        super(ConvColumn, self).__init__()\n",
        "\n",
        "        self.conv_layer1 = self._make_conv_layer(3, 64, (1, 2, 2), (1, 2, 2))\n",
        "        self.conv_layer2 = self._make_conv_layer(64, 128, (2, 2, 2), (2, 2, 2))\n",
        "        self.conv_layer3 = self._make_conv_layer(\n",
        "            128, 256, (2, 2, 2), (2, 2, 2))\n",
        "        self.conv_layer4 = self._make_conv_layer(\n",
        "            256, 256, (2, 2, 2), (2, 2, 2))\n",
        "\n",
        "        self.fc5 = nn.Linear(12800, 512)\n",
        "        self.fc5_act = nn.ELU()\n",
        "        self.fc6 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def _make_conv_layer(self, in_c, out_c, pool_size, stride):\n",
        "        conv_layer = nn.Sequential(\n",
        "            nn.Conv3d(in_c, out_c, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm3d(out_c),\n",
        "            nn.ELU(),\n",
        "            nn.MaxPool3d(pool_size, stride=stride, padding=0)\n",
        "        )\n",
        "        return conv_layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layer1(x)\n",
        "        x = self.conv_layer2(x)\n",
        "        x = self.conv_layer3(x)\n",
        "        x = self.conv_layer4(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = self.fc5(x)\n",
        "        x = self.fc5_act(x)\n",
        "\n",
        "        x = self.fc6(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTU-b0OqEm8l"
      },
      "source": [
        "device = torch.device('cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUHZwomwEm9F",
        "outputId": "a83eba08-8dd2-45bc-e0e4-55109e942141"
      },
      "source": [
        "model = ConvColumn(27)\n",
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConvColumn(\n",
              "  (conv_layer1): Sequential(\n",
              "    (0): Conv3d(3, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ELU(alpha=1.0)\n",
              "    (3): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv_layer2): Sequential(\n",
              "    (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "    (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ELU(alpha=1.0)\n",
              "    (3): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv_layer3): Sequential(\n",
              "    (0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "    (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ELU(alpha=1.0)\n",
              "    (3): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv_layer4): Sequential(\n",
              "    (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "    (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ELU(alpha=1.0)\n",
              "    (3): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (fc5): Linear(in_features=12800, out_features=512, bias=True)\n",
              "  (fc5_act): ELU(alpha=1.0)\n",
              "  (fc6): Linear(in_features=512, out_features=27, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o9dtxg-Em9l"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dycXAjCiEm-O"
      },
      "source": [
        "#define optimizer\n",
        "epochs = 50\n",
        "max_lr = 0.008\n",
        "lr = 0.001\n",
        "last_lr = 0.00001\n",
        "momentum = 0.9\n",
        "weight_decay = 0.00001\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr,\n",
        "                                momentum=momentum,\n",
        "                                weight_decay=weight_decay)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-_rL748Em_K"
      },
      "source": [
        "epochs = 50\n",
        "max_lr = 0.008\n",
        "grad_clip = 0.1\n",
        "weight_decay = 1e-4\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr, momentum=momentum, weight_decay=weight_decay)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMWEoZI1Em_R",
        "outputId": "0c82b630-e152-4022-f145-a798bb1c6fe7"
      },
      "source": [
        "print(torch.__version__)\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
        "                                                steps_per_epoch=len(train_loader))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLFEOALCEm_d"
      },
      "source": [
        "for batch_idx, (data, labels) in enumerate(train_loader):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6z66TO0Em_e",
        "outputId": "d83318b6-93c7-4e73-8020-177a6503b544"
      },
      "source": [
        "epoch_losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    time1 = time.time()\n",
        "    running_loss = 0.0\n",
        "    epoch_loss = []\n",
        "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
        "        data, labels = Variable(data), Variable(labels)\n",
        "        #data = data.cuda()\n",
        "        #labels = labels.cuda()\n",
        "        \n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        outputs = model(data)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        epoch_loss.append(loss.item())\n",
        "        if (batch_idx+1) % 100 == 99:\n",
        "            print(f'Train Epoch: {epoch+1}, Loss: {running_loss/100}')\n",
        "            time2 = time.time()\n",
        "            print(f'Spend time for 10000 images: {time2 - time1} sec')\n",
        "            time1 = time.time()\n",
        "            running_loss = 0.0\n",
        "    print(f'Epoch {epoch+1}, loss: ', np.mean(epoch_loss))\n",
        "    epoch_losses.append(epoch_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1, Loss: 1.6060215157270432\n",
            "Spend time for 10000 images: 2025.9248452186584 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xDfVUOdEm_m"
      },
      "source": [
        "losses = [np.mean(loss) for loss in epoch_losses]\n",
        "plt.plot(losses, '-x')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('losses')\n",
        "plt.title('losses vs. No. of epochs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVzPXYJyEm_2"
      },
      "source": [
        "torch.save(model.state_dict(), './models/proba.pth')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}